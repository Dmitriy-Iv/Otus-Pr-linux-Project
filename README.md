# ** Отказоустойчивый кластер серверов для сайта на базе WordPress**

В данном проекте расмотрена возможность создать простой отказоустойчивый кластер Web серверов для сайта на Wordpress.

- Для запуска данного проекта , необходимо скачать стенд и запустить его.

```
git clone https://github.com/Dmitriy-Iv/Otus-Pr-linux-Project.git
cd Otus-Pr-linux-Project/
vagrant up
```

Для того, чтобы открыть сайт и другие сервисы, необходимо в файл hosts на локальной машине внести следующие изменения:
```
192.168.56.10   wp.lab.local
192.168.56.10   grafana.lab.local
192.168.56.10   prometheus.lab.local
```

### **Схема и описание стенда**
![alt text](/screenshots/scheme.PNG?raw=true "Screenshot1")

Данный стенд состоит из 4 серверов, как и показано на схеме. Ниже поподробней расмотрим их настройку, согласно требованиям для проекта:

В роли Сервера, который является нашим бордером выступает сервер Testpr1. Он имеет два линка - во Outside (192.168.56.10) и в DMZ (10.10.62.10). На нём настроен iptables, который разрешает вход в периметр DMZ по https. В периметре DMZ у нас активны три линка:
- https://wp.lab.local/  - собственно наш сайт. Учётные данные: wpadmin, f7#swo56d@fLR3 (вход на https://wp.lab.local/wordpress/wp-admin/)
- https://grafana.lab.local/login - аналитика нашего мониторинга. Учётные данные: admin, HTdfg78345Kl+k3
- https://prometheus.lab.local/targets - наш мониторинг.

На данном сервере настроен Nginx(proxy), который проксирует запросы на один из двух серверов с Apache - Testpr2 и Testpr3. Для этого нам необходимы были самоподписанные сертификаты, которые при конфигурации Testpr1 генерируются скриптом на самом сервере и их пути прописаны в `nginx.conf`. Далее запросы уходят на сервера Apache, на которых в связке с PHP поднят Wordpress. Сервера два, за счёт этого происходит балансировка и резервирование нашей web части, то есть front-end.

В качестве back-end у нас выступает MySQL, который установлен также на серверах Testpr2 и Testpr3. Сервера настроены в режиме master-master репликации. Для отказоустойчивости на серверах установлен Keepalived, который держит VIP общий для MySQL. Таким образом у нас получается отказоустойчивость back-end. При выходе одного сервера back-end - Testpr2 и Testpr3 из строя (либо остановке просто службы Mysql), наш сайт продолжит работать, так как Nginx перенаправит новые запросы на работающий Apache. Работать наш сайт будет и в случае микса - потушим на одном сервере Apache, а на другом MySQL. Резервирование данных нашего сайта организовано с помощью rsync папок с данными между Testpr2 и Testpr3.

На сервере Testpr4 устновлен Prometheus (сервер мониторинга), и Grafana для построения графиков и визуализации мониторинга. На всех серверах установлены соотвественно node_exporter, а на Testpr2 и Testpr3 дополнительно mysql_exporter. На сервере Grafana скачаны несколько dashboards для этих expoter-ов. 

Также на сервере Testpr4 настроен Rsyslog(сервер логирования) сервер, на который сервера отправляют свои логи. Отправка сделана для Nginx, Apache, MySQL. 

Он же выступает и в роли бэкап сервера для серверов Testpr2 и Testpr3. На самих серверах в cron поставлен скрипт, который делает бэкап через mysqldump нашей базы, и копирует папку с данными нашего Wordpress. Далее копирует это уже на Testpr4 в папку с датой бэкапа, плюс пишет действия в файл лога на самих серверах. Так как жёсткого условия, что бэкап должен быть только через соотвествующую систему (как в ДЗ - Borg-backup), сделал через скрипт, но функционал бэкапа есть - на сервере есть файлы бэкапов по датам, нашёл/скопировал/развернул.

Соблюдено и условие - везде включен Selinux, для этого на серверах внесены соотвествующие изменения.

P.S.
Что не реализовано и можно было бы допилить:
1. Сделать два Nginx c HaProxy, чтобы сделать отказоустойчивость для Nginx,
2. Вместо Rsync сделать shared storage для данных WP(например папку на сервере для бэкапов Testpr4),
3. Если бы это был продакт, то бэкапить конечно не скриптом - а какой-нибудь системой бэкапов.
